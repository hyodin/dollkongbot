"""
RAG ì±„íŒ… API ë¼ìš°í„°

ì£¼ìš” ê¸°ëŠ¥:
1. ì§ˆë¬¸ ì •ê·œí™” (query_normalizer)
2. ë²¡í„° ê²€ìƒ‰ (Qdrant)
3. LLM ë‹µë³€ ìƒì„± (Gemini)
"""

import logging
import time
from typing import List, Dict, Any, Optional
from fastapi import APIRouter, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field

from services.vector_db import get_vector_db
from services.embedder import get_embedder
from services.safe_preprocessor import get_safe_preprocessor
from services.gemini_service import get_gemini_service, ChatMessage, initialize_gemini_service
from services.query_normalizer import get_query_normalizer  # ì§ˆë¬¸ ì •ê·œí™” ëª¨ë“ˆ

logger = logging.getLogger(__name__)

# ë¼ìš°í„° ì´ˆê¸°í™”
router = APIRouter(tags=["RAG Chat"])

# === ìš”ì²­/ì‘ë‹µ ëª¨ë¸ ===

class ChatRequest(BaseModel):
    """ì±„íŒ… ìš”ì²­ ëª¨ë¸"""
    question: str = Field(..., description="ì‚¬ìš©ì ì§ˆë¬¸", min_length=1, max_length=1000)
    use_context: bool = Field(True, description="ë¬¸ì„œ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš© ì—¬ë¶€")
    max_results: int = Field(5, description="ê²€ìƒ‰í•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜", ge=1, le=10)  # ë” ë§ì€ ê²°ê³¼
    score_threshold: float = Field(0.1, description="ë¬¸ì„œ ê²€ìƒ‰ ìµœì†Œ ì ìˆ˜", ge=0.0, le=1.0)  # ì„ê³„ê°’ ëŒ€í­ ë‚®ì¶¤
    max_tokens: int = Field(500, description="LLM ìµœëŒ€ ì‘ë‹µ í† í° ìˆ˜", ge=50, le=1000)  # í† í° ìˆ˜ ì¦ê°€ë¡œ ë” ìì„¸í•œ ë‹µë³€

class ContextDocument(BaseModel):
    """ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ ëª¨ë¸"""
    text: str = Field(..., description="ë¬¸ì„œ í…ìŠ¤íŠ¸")
    score: float = Field(..., description="ìœ ì‚¬ë„ ì ìˆ˜")
    source: str = Field(..., description="ë¬¸ì„œ ì¶œì²˜")
    metadata: Dict[str, Any] = Field(..., description="ë¬¸ì„œ ë©”íƒ€ë°ì´í„°")

class ChatResponse(BaseModel):
    """ì±„íŒ… ì‘ë‹µ ëª¨ë¸"""
    answer: str = Field(..., description="LLM ìƒì„± ë‹µë³€")
    question: str = Field(..., description="ì›ë³¸ ì§ˆë¬¸")
    context_used: bool = Field(..., description="ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš© ì—¬ë¶€")
    context_documents: List[ContextDocument] = Field(..., description="ì°¸ì¡°ëœ ë¬¸ì„œë“¤")
    model_info: Dict[str, Any] = Field(..., description="ì‚¬ìš©ëœ ëª¨ë¸ ì •ë³´")
    processing_time: Dict[str, float] = Field(..., description="ì²˜ë¦¬ ì‹œê°„ ë¶„ì„")
    token_usage: Dict[str, int] = Field(..., description="í† í° ì‚¬ìš©ëŸ‰")
    is_low_quality: bool = Field(False, description="ë‹µë³€ í’ˆì§ˆì´ ë‚®ì€ì§€ ì—¬ë¶€ (ë©”ì¼ ë¬¸ì˜ ë²„íŠ¼ í‘œì‹œìš©)")
    quality_score: float = Field(0.5, description="ë‹µë³€ í’ˆì§ˆ ì ìˆ˜ (0.0-1.0)")

class ChatHistoryRequest(BaseModel):
    """ì±„íŒ… íˆìŠ¤í† ë¦¬ ìš”ì²­ ëª¨ë¸"""
    messages: List[Dict[str, str]] = Field(..., description="ì±„íŒ… ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬")
    use_context: bool = Field(True, description="ë¬¸ì„œ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš© ì—¬ë¶€")
    max_results: int = Field(3, description="ê²€ìƒ‰í•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜", ge=1, le=10)
    score_threshold: float = Field(0.3, description="ë¬¸ì„œ ê²€ìƒ‰ ìµœì†Œ ì ìˆ˜", ge=0.0, le=1.0)

# === API ì—”ë“œí¬ì¸íŠ¸ ===

@router.post("/chat", response_model=ChatResponse)
async def chat_with_documents(request: ChatRequest):
    """
    ë¬¸ì„œ ê¸°ë°˜ RAG ì±„íŒ… API
    
    ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  LLMìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    start_time = time.time()
    
    try:
        logger.info(f"ğŸ“ RAG ì±„íŒ… ìš”ì²­: {request.question[:50]}...")
        
        # 1. LLM ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
        llm_service = get_gemini_service()
        if not llm_service:
            raise HTTPException(
                status_code=503, 
                detail="LLM ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ì„¸ìš”."
            )
            
        # í—¬ìŠ¤ì²´í¬ (ìºì‹±ëœ ê²°ê³¼ ì‚¬ìš©ìœ¼ë¡œ ì„±ëŠ¥ ê°œì„ )
        try:
            is_healthy = await llm_service.check_health()
            if not is_healthy:
                raise HTTPException(
                    status_code=503, 
                    detail="LLM ì„œë¹„ìŠ¤ë¥¼ ì¼ì‹œì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                )
        except Exception as e:
            logger.error(f"LLM í—¬ìŠ¤ì²´í¬ ì¤‘ ì˜¤ë¥˜: {e}")
            raise HTTPException(
                status_code=503, 
                detail="LLM ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            )
        
        # 1-1. ì§ˆë¬¸ ì˜ë„ ë¶„ë¥˜ (ì—…ë¬´/ì¼ìƒ/ì¸ì‚¬ 3ê°€ì§€ ë¶„ë¥˜)
        logger.info("=" * 70)
        logger.info("ğŸ¤– ì§ˆë¬¸ ì˜ë„ ë¶„ë¥˜ ì‹œì‘ (ì—…ë¬´/ì¼ìƒ/ì¸ì‚¬)")
        logger.info("=" * 70)
        intent_classification = await llm_service.classify_query_intent(request.question)
        intent_type = intent_classification.get("intent_type", "work")
        confidence = intent_classification.get("confidence", 0.0)
        reasoning = intent_classification.get("reasoning", "")
        
        logger.info(f"ğŸ“Š ì˜ë„ ë¶„ë¥˜ ê²°ê³¼:")
        logger.info(f"   - ì˜ë„ ìœ í˜•: {intent_type}")
        logger.info(f"   - ì‹ ë¢°ë„: {confidence:.2f}")
        if reasoning:
            logger.info(f"   - ì´ìœ : {reasoning}")
        
        # === ì¸ì‚¬ë§ ì²˜ë¦¬ ===
        if intent_type == "greeting" and confidence >= 0.5:
            logger.info("=" * 70)
            logger.info("ğŸ‘‹ ì¸ì‚¬ë§ë¡œ ë¶„ë¥˜ë¨ - ì¹œê·¼í•œ ì¸ì‚¬ ì‘ë‹µ ìƒì„±")
            logger.info("=" * 70)
            
            try:
                greeting_response = await llm_service.generate_greeting_response(request.question)
                total_time = time.time() - start_time
                
                response = ChatResponse(
                    answer=greeting_response["answer"],
                    question=request.question,
                    context_used=False,
                    context_documents=[],
                    model_info={
                        "llm_model": greeting_response["model"],
                        "embedding_model": "jhgan/ko-sbert-nli",
                        "vector_db": "qdrant"
                    },
                    processing_time={
                        "total": round(total_time, 3),
                        "search": 0.0,
                        "generation": round(total_time - start_time, 3),
                        "quality_evaluation": 0.0
                    },
                    token_usage={
                        "prompt_tokens": greeting_response["tokens_used"].get("input", 0),
                        "completion_tokens": greeting_response["tokens_used"].get("output", 0),
                        "total_tokens": greeting_response["tokens_used"].get("total", 0)
                    },
                    is_low_quality=False,
                    quality_score=1.0
                )
                
                logger.info(f"âœ… ì¸ì‚¬ë§ ì‘ë‹µ ì™„ë£Œ - ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
                return response
                
            except Exception as greeting_error:
                logger.error(f"âŒ ì¸ì‚¬ë§ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(greeting_error)}")
                # fallback ì¸ì‚¬ë§
                total_time = time.time() - start_time
                response = ChatResponse(
                    answer="ì•ˆë…•í•˜ì„¸ìš”! ëŒì½©ì´ì…ë‹ˆë‹¤ :) ì—…ë¬´ ê´€ë ¨í•´ì„œ ê¶ê¸ˆí•˜ì‹  ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë´ ì£¼ì„¸ìš”!",
                    question=request.question,
                    context_used=False,
                    context_documents=[],
                    model_info={
                        "llm_model": llm_service.model_name,
                        "embedding_model": "jhgan/ko-sbert-nli",
                        "vector_db": "qdrant"
                    },
                    processing_time={
                        "total": round(total_time, 3),
                        "search": 0.0,
                        "generation": 0.0,
                        "quality_evaluation": 0.0
                    },
                    token_usage={"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
                    is_low_quality=False,
                    quality_score=1.0
                )
                return response
        
        # === ì¼ìƒ ëŒ€í™” ì²˜ë¦¬ ===
        if intent_type == "casual" and confidence >= 0.5:
            logger.info("=" * 70)
            logger.info("ğŸ’¬ ì¼ìƒ ëŒ€í™”ë¡œ ë¶„ë¥˜ë¨ - ì•ˆë‚´ ë©”ì‹œì§€ ë°˜í™˜")
            logger.info("=" * 70)
            
            total_time = time.time() - start_time
            response = ChatResponse(
                answer="ì‚¬ë‚´ê·œì • ì „ë¬¸ê°€ë¡œì„œ ë“œë¦´ ë§ì”€ì´ ì—†êµ°ìš”.. ê·œì •ì— ëŒ€í•œ ì§ˆë¬¸ë§Œ í•´ì£¼ì„¸ìš” !ğŸ§",
                question=request.question,
                context_used=False,
                context_documents=[],
                model_info={
                    "llm_model": llm_service.model_name,
                    "embedding_model": "jhgan/ko-sbert-nli",
                    "vector_db": "qdrant"
                },
                processing_time={
                    "total": round(total_time, 3),
                    "search": 0.0,
                    "generation": 0.0,
                    "quality_evaluation": 0.0
                },
                token_usage={"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
                is_low_quality=False,
                quality_score=1.0
            )
            
            logger.info(f"âœ… ì¼ìƒ ëŒ€í™” ì•ˆë‚´ ë©”ì‹œì§€ ë°˜í™˜ ì™„ë£Œ - ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
            return response
        
        # === ì—…ë¬´ ì§ˆë¬¸ ì²˜ë¦¬ (ê¸°ì¡´ RAG í”Œë¡œìš°) ===
        logger.info("=" * 70)
        logger.info("ğŸ’¼ ì—…ë¬´ ì§ˆë¬¸ìœ¼ë¡œ ë¶„ë¥˜ë¨ - RAG í”Œë¡œìš° ì‹œì‘")
        logger.info("=" * 70)
        
        search_time_start = time.time()
        context_documents = []
        
        # 2. ë¬¸ì„œ ê²€ìƒ‰ (ì—…ë¬´ ì§ˆë¬¸ì¼ ë•Œë§Œ)
        if request.use_context:
            try:
                logger.info("=" * 70)
                logger.info(f"ğŸ” RAG ê²€ìƒ‰ ì‹œì‘")
                logger.info("=" * 70)
                logger.info(f"ì›ë³¸ ì§ˆë¬¸: '{request.question}'")
                
                # ============================================================
                # Step 2-1: ì§ˆë¬¸ ì •ê·œí™” (ìƒˆë¡œ ì¶”ê°€!)
                # ============================================================
                logger.info("â”" * 60)
                logger.info("Step 2-1: ì§ˆë¬¸ ì •ê·œí™” í”„ë¡œì„¸ìŠ¤")
                logger.info("â”" * 60)
                
                try:
                    normalizer = get_query_normalizer()
                    processed_query = normalizer.normalize(request.question)
                    
                    logger.info(f"âœ… ì§ˆë¬¸ ì •ê·œí™” ì™„ë£Œ")
                    logger.info(f"   ì›ë³¸: '{request.question}'")
                    logger.info(f"   ì •ê·œí™”: '{processed_query}'")
                    
                    # ì •ê·œí™” ê²°ê³¼ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
                    if len(processed_query.strip()) < 2:
                        logger.warning("âš  ì •ê·œí™” ê²°ê³¼ê°€ ë„ˆë¬´ ì§§ìŒ - ì›ë³¸ ì‚¬ìš©")
                        processed_query = request.question.strip()
                    
                    # ì •ê·œí™” í†µê³„ ë¡œê¹…
                    stats = normalizer.get_stats()
                    logger.debug(f"ì •ê·œí™” í†µê³„: {stats}")
                    
                except Exception as norm_error:
                    logger.error(f"âŒ ì§ˆë¬¸ ì •ê·œí™” ì‹¤íŒ¨: {str(norm_error)}")
                    logger.warning("âš  ì›ë³¸ ì§ˆë¬¸ ì‚¬ìš© (fallback)")
                    processed_query = request.question.strip()
                
                # ============================================================
                # Step 2-2: ìµœì¢… ì¿¼ë¦¬ ì¤€ë¹„
                # ============================================================
                final_query = processed_query
                logger.info(f"âœ“ ìµœì¢… ê²€ìƒ‰ ì¿¼ë¦¬: '{final_query}'")
                
                # ì„ë² ë”© ìƒì„±
                logger.info("ğŸ§  ì„ë² ë”© ìƒì„± ì‹œì‘...")
                embedder = get_embedder()
                query_embedding = embedder.encode_text(final_query)  # ì˜¬ë°”ë¥¸ ë©”ì„œë“œ í˜¸ì¶œ
                logger.info(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ - ì°¨ì›: {query_embedding.shape}")
                
                # Qdrant DB ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
                logger.info(f"ğŸ” Qdrant DB ê²€ìƒ‰ ì‹œì‘ - ì§ˆë¬¸: '{request.question[:50]}...'")
                vector_db = get_vector_db()
                search_results = vector_db.search_similar(
                    query_embedding=query_embedding,
                    limit=request.max_results,
                    score_threshold=request.score_threshold
                )
                
                logger.info(f"ğŸ“Š Qdrant DB ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ ë¬¸ì„œ ë°œê²¬")
                if search_results:
                    for i, result in enumerate(search_results[:3]):  # ìƒìœ„ 3ê°œë§Œ ë¡œê¹…
                        logger.info(f"  {i+1}. {result['metadata']['file_name']} (ì ìˆ˜: {result['score']:.3f})")
                        logger.info(f"      ë‚´ìš©: {result['text'][:100]}...")
                    
                    # ì ìˆ˜ ê¸°ë°˜ ì •ë ¬
                    search_results = sorted(search_results, key=lambda x: x.get("score", 0), reverse=True)
                else:
                    logger.error(f"âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ! íŒŒë¼ë¯¸í„°: limit={request.max_results}, threshold={request.score_threshold}")
                    # ì„ê³„ê°’ì„ ë” ë‚®ì¶°ì„œ ì¬ì‹œë„
                    logger.info("ğŸ”„ ì„ê³„ê°’ì„ 0.05ë¡œ ë‚®ì¶°ì„œ ì¬ê²€ìƒ‰ ì‹œë„...")
                    search_results = vector_db.search_similar(
                        query_embedding=query_embedding,
                        limit=request.max_results,
                        score_threshold=0.05
                    )
                    logger.info(f"ğŸ”„ ì¬ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ ë¬¸ì„œ")
                
                # ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ ë³€í™˜
                for result in search_results:
                    context_doc = ContextDocument(
                        text=result["text"],
                        score=result["score"],
                        source=_format_source_info(result["metadata"]),
                        metadata=result["metadata"]
                    )
                    context_documents.append(context_doc)
                
                logger.info(f"ğŸ” ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ: {len(context_documents)}ê°œ ë¬¸ì„œ ë°œê²¬")
                
            except Exception as e:
                logger.error(f"âŒ ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}", exc_info=True)
                logger.error(f"ê²€ìƒ‰ íŒŒë¼ë¯¸í„°: query='{request.question}', limit={request.max_results}, threshold={request.score_threshold}")
        
        search_time = time.time() - search_time_start
        
        # 3. LLM ë‹µë³€ ìƒì„±
        generation_time_start = time.time()
        
        # ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
        context_docs_dict = [doc.dict() for doc in context_documents] if context_documents else None
        
        if context_docs_dict:
            logger.info(f"ğŸ¤– LLMì— ì „ë‹¬í•  ì»¨í…ìŠ¤íŠ¸: {len(context_docs_dict)}ê°œ ë¬¸ì„œ")
            for i, doc in enumerate(context_docs_dict[:2]):  # ìƒìœ„ 2ê°œë§Œ ë¡œê¹…
                logger.info(f"  ì»¨í…ìŠ¤íŠ¸ {i+1}: {doc['text'][:100]}...")
        else:
            logger.info("ğŸ¤– LLM ì»¨í…ìŠ¤íŠ¸ ì—†ì´ ë‹µë³€ ìƒì„±")
        
        llm_response = await llm_service.generate_response(
            question=request.question,
            context_documents=context_docs_dict,
            max_tokens=request.max_tokens
        )
        
        generation_time = time.time() - generation_time_start
        
        # 4. ë‹µë³€ í’ˆì§ˆ í‰ê°€
        quality_evaluation_start = time.time()
        is_low_quality = False
        quality_score = 0.5
        
        try:
            logger.info("=" * 70)
            logger.info("ğŸ“Š ë‹µë³€ í’ˆì§ˆ í‰ê°€ ì‹œì‘")
            logger.info("=" * 70)
            
            quality_result = await llm_service.evaluate_response_quality(
                question=request.question,
                answer=llm_response["answer"],
                context_documents=context_docs_dict
            )
            
            is_low_quality = quality_result.get("is_low_quality", False)
            quality_score = quality_result.get("quality_score", 0.5)
            quality_reason = quality_result.get("reason", "")
            
            logger.info(f"âœ… í’ˆì§ˆ í‰ê°€ ì™„ë£Œ: is_low_quality={is_low_quality}, score={quality_score:.2f}")
            if quality_reason:
                logger.info(f"   ì´ìœ : {quality_reason}")
        except Exception as quality_error:
            logger.warning(f"âš  ë‹µë³€ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {str(quality_error)}")
            # í‰ê°€ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš© (ë‚®ì€ í’ˆì§ˆë¡œ ê°„ì£¼)
            is_low_quality = False
            quality_score = 0.5
        
        quality_evaluation_time = time.time() - quality_evaluation_start
        total_time = time.time() - start_time
        
        # 5. ì‘ë‹µ êµ¬ì„±
        response = ChatResponse(
            answer=llm_response["answer"],  # "response" -> "answer"ë¡œ ìˆ˜ì •
            question=request.question,
            context_used=request.use_context and len(context_documents) > 0,
            context_documents=context_documents,
            model_info={
                "llm_model": llm_response["model"],
                "embedding_model": "jhgan/ko-sbert-nli",
                "vector_db": "qdrant"
            },
            processing_time={
                "total": round(total_time, 3),
                "search": round(search_time, 3),
                "generation": round(generation_time, 3),
                "quality_evaluation": round(quality_evaluation_time, 3)
            },
            token_usage={
                "prompt_tokens": llm_response.get("tokens_used", {}).get("input", 0),
                "completion_tokens": llm_response.get("tokens_used", {}).get("output", 0),
                "total_tokens": llm_response.get("tokens_used", {}).get("total", 0)
            },
            is_low_quality=is_low_quality,
            quality_score=quality_score
        )
        
        logger.info(f"âœ… RAG ì±„íŒ… ì™„ë£Œ - ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        total_time = time.time() - start_time
        logger.error(f"âŒ RAG ì±„íŒ… ì²˜ë¦¬ ì‹¤íŒ¨ (ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ): {str(e)}")
        
        # ì˜¤ë¥˜ ìœ í˜•ë³„ ìƒì„¸ ë¡œê¹…
        import traceback
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:\n{traceback.format_exc()}")
        
        # ì‚¬ìš©ì ì¹œí™”ì  ì˜¤ë¥˜ ë©”ì‹œì§€
        error_msg = "ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        if "timeout" in str(e).lower():
            error_msg = "ì‘ë‹µ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        elif "api" in str(e).lower():
            error_msg = "AI ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        elif "network" in str(e).lower():
            error_msg = "ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        
        raise HTTPException(
            status_code=500, 
            detail=error_msg
        )

@router.post("/chat/history", response_model=ChatResponse)
async def chat_with_history(request: ChatHistoryRequest):
    """
    ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ RAG ì±„íŒ… API
    
    ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ë¥¼ ChatMessage ê°ì²´ë¡œ ë³€í™˜
        chat_messages = []
        for msg in request.messages:
            if "role" in msg and "content" in msg:
                chat_messages.append(ChatMessage(
                    role=msg["role"],
                    content=msg["content"]
                ))
        
        if not chat_messages:
            raise HTTPException(status_code=400, detail="ì±„íŒ… ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
        user_messages = [msg for msg in chat_messages if msg.role == "user"]
        if not user_messages:
            raise HTTPException(status_code=400, detail="ì‚¬ìš©ì ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        last_question = user_messages[-1].content
        
        # ë‹¨ìˆœ ì±„íŒ… ìš”ì²­ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì²˜ë¦¬ (í–¥í›„ íˆìŠ¤í† ë¦¬ ì§€ì› í™•ì¥ ê°€ëŠ¥)
        simple_request = ChatRequest(
            question=last_question,
            use_context=request.use_context,
            max_results=request.max_results,
            score_threshold=request.score_threshold
        )
        
        return await chat_with_documents(simple_request)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ íˆìŠ¤í† ë¦¬ ì±„íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(
            status_code=500, 
            detail=f"íˆìŠ¤í† ë¦¬ ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

@router.get("/chat/health")
async def check_chat_health():
    """
    RAG ì±„íŒ… ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
    """
    try:
        # LLM ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
        llm_service = get_gemini_service()
        llm_healthy = await llm_service.check_health()
        
        # ë²¡í„° DB ìƒíƒœ í™•ì¸
        try:
            vector_db = get_vector_db()
            collections = vector_db.client.get_collections()
            vector_db_healthy = True
        except Exception:
            vector_db_healthy = False
        
        # ì„ë² ë”© ëª¨ë¸ ìƒíƒœ í™•ì¸
        try:
            embedder = get_embedder()
            embedding_healthy = embedder.model is not None
        except Exception:
            embedding_healthy = False
        
        overall_health = llm_healthy and vector_db_healthy and embedding_healthy
        
        return {
            "status": "healthy" if overall_health else "degraded",
            "services": {
                "llm": "online" if llm_healthy else "offline",
                "vector_db": "online" if vector_db_healthy else "offline", 
                "embedder": "online" if embedding_healthy else "offline"
            },
            "capabilities": {
                "rag_chat": overall_health,
                "document_search": vector_db_healthy and embedding_healthy,
                "llm_generation": llm_healthy
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ ì±„íŒ… ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}")
        return {
            "status": "error",
            "error": str(e)
        }

# === í—¬í¼ í•¨ìˆ˜ ===

def _format_source_info(metadata: Dict[str, Any]) -> str:
    """ì¶œì²˜ ì •ë³´ í¬ë§·íŒ…"""
    file_name = metadata.get("file_name", "ì•Œ ìˆ˜ ì—†ìŒ")
    sheet_name = metadata.get("sheet_name")
    
    if sheet_name:
        return f"{file_name} > {sheet_name} ì‹œíŠ¸"
    else:
        return file_name

