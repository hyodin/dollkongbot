"""
RAG Ï±ÑÌåÖ API ÎùºÏö∞ÌÑ∞
"""

import logging
import time
from typing import List, Dict, Any, Optional
from fastapi import APIRouter, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field

from services.vector_db import get_vector_db
from services.embedder import get_embedder
from services.safe_preprocessor import get_safe_preprocessor
from services.gemini_service import get_gemini_service, ChatMessage, initialize_gemini_service

logger = logging.getLogger(__name__)

# ÎùºÏö∞ÌÑ∞ Ï¥àÍ∏∞Ìôî
router = APIRouter(prefix="/api", tags=["RAG Chat"])

# === ÏöîÏ≤≠/ÏùëÎãµ Î™®Îç∏ ===

class ChatRequest(BaseModel):
    """Ï±ÑÌåÖ ÏöîÏ≤≠ Î™®Îç∏"""
    question: str = Field(..., description="ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏", min_length=1, max_length=1000)
    use_context: bool = Field(True, description="Î¨∏ÏÑú Í≤ÄÏÉâ Ïª®ÌÖçÏä§Ìä∏ ÏÇ¨Ïö© Ïó¨Î∂Ä")
    max_results: int = Field(5, description="Í≤ÄÏÉâÌï† ÏµúÎåÄ Î¨∏ÏÑú Ïàò", ge=1, le=10)  # Îçî ÎßéÏùÄ Í≤∞Í≥º
    score_threshold: float = Field(0.1, description="Î¨∏ÏÑú Í≤ÄÏÉâ ÏµúÏÜå Ï†êÏàò", ge=0.0, le=1.0)  # ÏûÑÍ≥ÑÍ∞í ÎåÄÌè≠ ÎÇÆÏ∂§
    max_tokens: int = Field(500, description="LLM ÏµúÎåÄ ÏùëÎãµ ÌÜ†ÌÅ∞ Ïàò", ge=50, le=1000)  # ÌÜ†ÌÅ∞ Ïàò Ï¶ùÍ∞ÄÎ°ú Îçî ÏûêÏÑ∏Ìïú ÎãµÎ≥Ä

class ContextDocument(BaseModel):
    """Ïª®ÌÖçÏä§Ìä∏ Î¨∏ÏÑú Î™®Îç∏"""
    text: str = Field(..., description="Î¨∏ÏÑú ÌÖçÏä§Ìä∏")
    score: float = Field(..., description="Ïú†ÏÇ¨ÎèÑ Ï†êÏàò")
    source: str = Field(..., description="Î¨∏ÏÑú Ï∂úÏ≤ò")
    metadata: Dict[str, Any] = Field(..., description="Î¨∏ÏÑú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞")

class ChatResponse(BaseModel):
    """Ï±ÑÌåÖ ÏùëÎãµ Î™®Îç∏"""
    answer: str = Field(..., description="LLM ÏÉùÏÑ± ÎãµÎ≥Ä")
    question: str = Field(..., description="ÏõêÎ≥∏ ÏßàÎ¨∏")
    context_used: bool = Field(..., description="Î¨∏ÏÑú Ïª®ÌÖçÏä§Ìä∏ ÏÇ¨Ïö© Ïó¨Î∂Ä")
    context_documents: List[ContextDocument] = Field(..., description="Ï∞∏Ï°∞Îêú Î¨∏ÏÑúÎì§")
    model_info: Dict[str, Any] = Field(..., description="ÏÇ¨Ïö©Îêú Î™®Îç∏ Ï†ïÎ≥¥")
    processing_time: Dict[str, float] = Field(..., description="Ï≤òÎ¶¨ ÏãúÍ∞Ñ Î∂ÑÏÑù")
    token_usage: Dict[str, int] = Field(..., description="ÌÜ†ÌÅ∞ ÏÇ¨Ïö©Îüâ")

class ChatHistoryRequest(BaseModel):
    """Ï±ÑÌåÖ ÌûàÏä§ÌÜ†Î¶¨ ÏöîÏ≤≠ Î™®Îç∏"""
    messages: List[Dict[str, str]] = Field(..., description="Ï±ÑÌåÖ Î©îÏãúÏßÄ ÌûàÏä§ÌÜ†Î¶¨")
    use_context: bool = Field(True, description="Î¨∏ÏÑú Í≤ÄÏÉâ Ïª®ÌÖçÏä§Ìä∏ ÏÇ¨Ïö© Ïó¨Î∂Ä")
    max_results: int = Field(3, description="Í≤ÄÏÉâÌï† ÏµúÎåÄ Î¨∏ÏÑú Ïàò", ge=1, le=10)
    score_threshold: float = Field(0.3, description="Î¨∏ÏÑú Í≤ÄÏÉâ ÏµúÏÜå Ï†êÏàò", ge=0.0, le=1.0)

# === API ÏóîÎìúÌè¨Ïù∏Ìä∏ ===

@router.post("/chat", response_model=ChatResponse)
async def chat_with_documents(request: ChatRequest):
    """
    Î¨∏ÏÑú Í∏∞Î∞ò RAG Ï±ÑÌåÖ API
    
    ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏Ïóê ÎåÄÌï¥ Í¥ÄÎ†® Î¨∏ÏÑúÎ•º Í≤ÄÏÉâÌïòÍ≥† LLMÏúºÎ°ú ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.
    """
    start_time = time.time()
    
    try:
        logger.info(f"üìù RAG Ï±ÑÌåÖ ÏöîÏ≤≠: {request.question[:50]}...")
        
        # 1. LLM ÏÑúÎπÑÏä§ ÏÉÅÌÉú ÌôïÏù∏
        llm_service = get_gemini_service()
        if not llm_service:
            raise HTTPException(
                status_code=503, 
                detail="LLM ÏÑúÎπÑÏä§Í∞Ä Ï¥àÍ∏∞ÌôîÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. ÏÑúÎ≤ÑÎ•º Ïû¨ÏãúÏûëÌïòÏÑ∏Ïöî."
            )
            
        # Ìó¨Ïä§Ï≤¥ÌÅ¨ (Ï∫êÏã±Îêú Í≤∞Í≥º ÏÇ¨Ïö©ÏúºÎ°ú ÏÑ±Îä• Í∞úÏÑ†)
        try:
            is_healthy = await llm_service.check_health()
            if not is_healthy:
                raise HTTPException(
                    status_code=503, 
                    detail="LLM ÏÑúÎπÑÏä§Î•º ÏùºÏãúÏ†ÅÏúºÎ°ú ÏÇ¨Ïö©Ìï† Ïàò ÏóÜÏäµÎãàÎã§. Ïû†Ïãú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî."
                )
        except Exception as e:
            logger.error(f"LLM Ìó¨Ïä§Ï≤¥ÌÅ¨ Ï§ë Ïò§Î•ò: {e}")
            raise HTTPException(
                status_code=503, 
                detail="LLM ÏÑúÎπÑÏä§ ÏÉÅÌÉú ÌôïÏù∏ Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. Ïû†Ïãú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî."
            )
        
        search_time_start = time.time()
        context_documents = []
        
        # 2. Î¨∏ÏÑú Í≤ÄÏÉâ (Ïª®ÌÖçÏä§Ìä∏ ÏÇ¨Ïö© Ïãú)
        if request.use_context:
            try:
                logger.info(f"üîç RAG Í≤ÄÏÉâ ÏãúÏûë - ÏõêÎ≥∏ ÏßàÎ¨∏: '{request.question}'")
                
                # Ï†ÑÏ≤òÎ¶¨ Í±¥ÎÑàÎõ∞Í≥† ÏõêÎ≥∏ ÏßàÎ¨∏ ÏßÅÏ†ë ÏÇ¨Ïö© (Î¨∏Ï†ú Ìï¥Í≤∞ÏùÑ ÏúÑÌï¥)
                processed_query = request.question.strip()
                logger.info(f"üìù ÏõêÎ≥∏ ÏßàÎ¨∏ ÏßÅÏ†ë ÏÇ¨Ïö©: '{processed_query}'")
                
                # ÌÇ§ÏõåÎìú ÌôïÏû• Í±¥ÎÑàÎõ∞Í≥† ÏßÅÏ†ë ÏûÑÎ≤†Îî© (Î¨∏Ï†ú Ìï¥Í≤∞ÏùÑ ÏúÑÌï¥)
                final_query = processed_query
                logger.info(f"üîç ÏµúÏ¢Ö Í≤ÄÏÉâ ÏßàÎ¨∏: '{final_query}'")
                
                # ÏûÑÎ≤†Îî© ÏÉùÏÑ±
                logger.info("üß† ÏûÑÎ≤†Îî© ÏÉùÏÑ± ÏãúÏûë...")
                embedder = get_embedder()
                query_embedding = embedder.encode_text(final_query)  # Ïò¨Î∞îÎ•∏ Î©îÏÑúÎìú Ìò∏Ï∂ú
                logger.info(f"‚úÖ ÏûÑÎ≤†Îî© ÏÉùÏÑ± ÏôÑÎ£å - Ï∞®Ïõê: {query_embedding.shape}")
                
                # Qdrant DB Î≤°ÌÑ∞ Í≤ÄÏÉâ ÏàòÌñâ
                logger.info(f"üîç Qdrant DB Í≤ÄÏÉâ ÏãúÏûë - ÏßàÎ¨∏: '{request.question[:50]}...'")
                vector_db = get_vector_db()
                search_results = vector_db.search_similar(
                    query_embedding=query_embedding,
                    limit=request.max_results,
                    score_threshold=request.score_threshold
                )
                
                logger.info(f"üìä Qdrant DB Í≤ÄÏÉâ Í≤∞Í≥º: {len(search_results)}Í∞ú Î¨∏ÏÑú Î∞úÍ≤¨")
                if search_results:
                    for i, result in enumerate(search_results[:3]):  # ÏÉÅÏúÑ 3Í∞úÎßå Î°úÍπÖ
                        logger.info(f"  {i+1}. {result['metadata']['file_name']} (Ï†êÏàò: {result['score']:.3f})")
                        logger.info(f"      ÎÇ¥Ïö©: {result['text'][:100]}...")
                    
                    # Ï†êÏàò Í∏∞Î∞ò Ï†ïÎ†¨
                    search_results = sorted(search_results, key=lambda x: x.get("score", 0), reverse=True)
                else:
                    logger.error(f"‚ùå Í≤ÄÏÉâ Í≤∞Í≥º ÏóÜÏùå! ÌååÎùºÎØ∏ÌÑ∞: limit={request.max_results}, threshold={request.score_threshold}")
                    # ÏûÑÍ≥ÑÍ∞íÏùÑ Îçî ÎÇÆÏ∂∞ÏÑú Ïû¨ÏãúÎèÑ
                    logger.info("üîÑ ÏûÑÍ≥ÑÍ∞íÏùÑ 0.05Î°ú ÎÇÆÏ∂∞ÏÑú Ïû¨Í≤ÄÏÉâ ÏãúÎèÑ...")
                    search_results = vector_db.search_similar(
                        query_embedding=query_embedding,
                        limit=request.max_results,
                        score_threshold=0.05
                    )
                    logger.info(f"üîÑ Ïû¨Í≤ÄÏÉâ Í≤∞Í≥º: {len(search_results)}Í∞ú Î¨∏ÏÑú")
                
                # Ïª®ÌÖçÏä§Ìä∏ Î¨∏ÏÑú Î≥ÄÌôò
                for result in search_results:
                    context_doc = ContextDocument(
                        text=result["text"],
                        score=result["score"],
                        source=_format_source_info(result["metadata"]),
                        metadata=result["metadata"]
                    )
                    context_documents.append(context_doc)
                
                logger.info(f"üîç Î¨∏ÏÑú Í≤ÄÏÉâ ÏôÑÎ£å: {len(context_documents)}Í∞ú Î¨∏ÏÑú Î∞úÍ≤¨")
                
            except Exception as e:
                logger.error(f"‚ùå Î¨∏ÏÑú Í≤ÄÏÉâ Ïã§Ìå®: {str(e)}", exc_info=True)
                logger.error(f"Í≤ÄÏÉâ ÌååÎùºÎØ∏ÌÑ∞: query='{request.question}', limit={request.max_results}, threshold={request.score_threshold}")
        
        search_time = time.time() - search_time_start
        
        # 3. LLM ÎãµÎ≥Ä ÏÉùÏÑ±
        generation_time_start = time.time()
        
        # Ïª®ÌÖçÏä§Ìä∏ Î¨∏ÏÑúÎ•º ÎîïÏÖîÎÑàÎ¶¨ ÌòïÌÉúÎ°ú Î≥ÄÌôò
        context_docs_dict = [doc.dict() for doc in context_documents] if context_documents else None
        
        if context_docs_dict:
            logger.info(f"ü§ñ LLMÏóê Ï†ÑÎã¨Ìï† Ïª®ÌÖçÏä§Ìä∏: {len(context_docs_dict)}Í∞ú Î¨∏ÏÑú")
            for i, doc in enumerate(context_docs_dict[:2]):  # ÏÉÅÏúÑ 2Í∞úÎßå Î°úÍπÖ
                logger.info(f"  Ïª®ÌÖçÏä§Ìä∏ {i+1}: {doc['text'][:100]}...")
        else:
            logger.info("ü§ñ LLM Ïª®ÌÖçÏä§Ìä∏ ÏóÜÏù¥ ÎãµÎ≥Ä ÏÉùÏÑ±")
        
        llm_response = await llm_service.generate_response(
            question=request.question,
            context_documents=context_docs_dict,
            max_tokens=request.max_tokens
        )
        
        generation_time = time.time() - generation_time_start
        total_time = time.time() - start_time
        
        # 4. ÏùëÎãµ Íµ¨ÏÑ±
        response = ChatResponse(
            answer=llm_response["answer"],  # "response" -> "answer"Î°ú ÏàòÏ†ï
            question=request.question,
            context_used=request.use_context and len(context_documents) > 0,
            context_documents=context_documents,
            model_info={
                "llm_model": llm_response["model"],
                "embedding_model": "jhgan/ko-sbert-nli",
                "vector_db": "qdrant"
            },
            processing_time={
                "total": round(total_time, 3),
                "search": round(search_time, 3),
                "generation": round(generation_time, 3)
            },
            token_usage={
                "prompt_tokens": llm_response.get("tokens_used", {}).get("input", 0),
                "completion_tokens": llm_response.get("tokens_used", {}).get("output", 0),
                "total_tokens": llm_response.get("tokens_used", {}).get("total", 0)
            }
        )
        
        logger.info(f"‚úÖ RAG Ï±ÑÌåÖ ÏôÑÎ£å - Ï¥ù Ï≤òÎ¶¨ ÏãúÍ∞Ñ: {total_time:.2f}Ï¥à")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        total_time = time.time() - start_time
        logger.error(f"‚ùå RAG Ï±ÑÌåÖ Ï≤òÎ¶¨ Ïã§Ìå® (Ï≤òÎ¶¨ ÏãúÍ∞Ñ: {total_time:.2f}Ï¥à): {str(e)}")
        
        # Ïò§Î•ò Ïú†ÌòïÎ≥Ñ ÏÉÅÏÑ∏ Î°úÍπÖ
        import traceback
        logger.error(f"ÏÉÅÏÑ∏ Ïò§Î•ò Ï†ïÎ≥¥:\n{traceback.format_exc()}")
        
        # ÏÇ¨Ïö©Ïûê ÏπúÌôîÏ†Å Ïò§Î•ò Î©îÏãúÏßÄ
        error_msg = "Ï±ÑÌåÖ Ï≤òÎ¶¨ Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§."
        if "timeout" in str(e).lower():
            error_msg = "ÏùëÎãµ ÏãúÍ∞ÑÏù¥ Ï¥àÍ≥ºÎêòÏóàÏäµÎãàÎã§. Ïû†Ïãú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî."
        elif "api" in str(e).lower():
            error_msg = "AI ÏÑúÎπÑÏä§Ïóê ÏùºÏãúÏ†ÅÏù∏ Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. Ïû†Ïãú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî."
        elif "network" in str(e).lower():
            error_msg = "ÎÑ§Ìä∏ÏõåÌÅ¨ Ïó∞Í≤∞Ïóê Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. Ïû†Ïãú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî."
        
        raise HTTPException(
            status_code=500, 
            detail=error_msg
        )

@router.post("/chat/history", response_model=ChatResponse)
async def chat_with_history(request: ChatHistoryRequest):
    """
    Ï±ÑÌåÖ ÌûàÏä§ÌÜ†Î¶¨Î•º Ìè¨Ìï®Ìïú RAG Ï±ÑÌåÖ API
    
    Ïù¥Ï†Ñ ÎåÄÌôî ÎÇ¥Ïö©ÏùÑ Í≥†Î†§ÌïòÏó¨ ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.
    """
    try:
        # Î©îÏãúÏßÄ ÌûàÏä§ÌÜ†Î¶¨Î•º ChatMessage Í∞ùÏ≤¥Î°ú Î≥ÄÌôò
        chat_messages = []
        for msg in request.messages:
            if "role" in msg and "content" in msg:
                chat_messages.append(ChatMessage(
                    role=msg["role"],
                    content=msg["content"]
                ))
        
        if not chat_messages:
            raise HTTPException(status_code=400, detail="Ï±ÑÌåÖ Î©îÏãúÏßÄÍ∞Ä ÏóÜÏäµÎãàÎã§")
        
        # ÎßàÏßÄÎßâ ÏÇ¨Ïö©Ïûê Î©îÏãúÏßÄ Ï∂îÏ∂ú
        user_messages = [msg for msg in chat_messages if msg.role == "user"]
        if not user_messages:
            raise HTTPException(status_code=400, detail="ÏÇ¨Ïö©Ïûê Î©îÏãúÏßÄÍ∞Ä ÏóÜÏäµÎãàÎã§")
        
        last_question = user_messages[-1].content
        
        # Îã®Ïàú Ï±ÑÌåÖ ÏöîÏ≤≠ÏúºÎ°ú Î≥ÄÌôòÌïòÏó¨ Ï≤òÎ¶¨ (Ìñ•ÌõÑ ÌûàÏä§ÌÜ†Î¶¨ ÏßÄÏõê ÌôïÏû• Í∞ÄÎä•)
        simple_request = ChatRequest(
            question=last_question,
            use_context=request.use_context,
            max_results=request.max_results,
            score_threshold=request.score_threshold
        )
        
        return await chat_with_documents(simple_request)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå ÌûàÏä§ÌÜ†Î¶¨ Ï±ÑÌåÖ Ï≤òÎ¶¨ Ïã§Ìå®: {str(e)}")
        raise HTTPException(
            status_code=500, 
            detail=f"ÌûàÏä§ÌÜ†Î¶¨ Ï±ÑÌåÖ Ï≤òÎ¶¨ Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}"
        )

@router.get("/chat/health")
async def check_chat_health():
    """
    RAG Ï±ÑÌåÖ ÏãúÏä§ÌÖú ÏÉÅÌÉú ÌôïÏù∏
    """
    try:
        # LLM ÏÑúÎπÑÏä§ ÏÉÅÌÉú ÌôïÏù∏
        llm_service = get_gemini_service()
        llm_healthy = await llm_service.check_health()
        
        # Î≤°ÌÑ∞ DB ÏÉÅÌÉú ÌôïÏù∏
        try:
            vector_db = get_vector_db()
            collections = vector_db.client.get_collections()
            vector_db_healthy = True
        except Exception:
            vector_db_healthy = False
        
        # ÏûÑÎ≤†Îî© Î™®Îç∏ ÏÉÅÌÉú ÌôïÏù∏
        try:
            embedder = get_embedder()
            embedding_healthy = embedder.model is not None
        except Exception:
            embedding_healthy = False
        
        overall_health = llm_healthy and vector_db_healthy and embedding_healthy
        
        return {
            "status": "healthy" if overall_health else "degraded",
            "services": {
                "llm": "online" if llm_healthy else "offline",
                "vector_db": "online" if vector_db_healthy else "offline", 
                "embedder": "online" if embedding_healthy else "offline"
            },
            "capabilities": {
                "rag_chat": overall_health,
                "document_search": vector_db_healthy and embedding_healthy,
                "llm_generation": llm_healthy
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå Ï±ÑÌåÖ ÏãúÏä§ÌÖú ÏÉÅÌÉú ÌôïÏù∏ Ïã§Ìå®: {str(e)}")
        return {
            "status": "error",
            "error": str(e)
        }

# === Ìó¨Ìçº Ìï®Ïàò ===

def _format_source_info(metadata: Dict[str, Any]) -> str:
    """Ï∂úÏ≤ò Ï†ïÎ≥¥ Ìè¨Îß∑ÌåÖ"""
    file_name = metadata.get("file_name", "Ïïå Ïàò ÏóÜÏùå")
    sheet_name = metadata.get("sheet_name")
    
    if sheet_name:
        return f"{file_name} > {sheet_name} ÏãúÌä∏"
    else:
        return file_name

