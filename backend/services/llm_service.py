"""
Ollama LLM ì„œë¹„ìŠ¤ - Gemma-2-9B-IT ëª¨ë¸ ì—°ë™
"""

import logging
import asyncio
import aiohttp
import json
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class ChatMessage:
    """ì±„íŒ… ë©”ì‹œì§€ ëª¨ë¸"""
    role: str  # "user", "assistant", "system"
    content: str

class OllamaLLMService:
    """Ollama LLM ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 host: str = "localhost", 
                 port: int = 11434, 
                 model: str = "qwen2:7b",
                 timeout: int = 15):
        """
        Ollama LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            host: Ollama ì„œë²„ í˜¸ìŠ¤íŠ¸
            port: Ollama ì„œë²„ í¬íŠ¸  
            model: ì‚¬ìš©í•  ëª¨ë¸ëª… (qwen2:7b)
            timeout: ì‘ë‹µ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
        """
        self.host = host
        self.port = port
        self.model = model
        self.timeout = timeout
        self.base_url = f"http://{host}:{port}"
        
        logger.info(f"OllamaLLMService ì´ˆê¸°í™” - ëª¨ë¸: {model}, URL: {self.base_url}")

    async def check_health(self) -> bool:
        """Ollama ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                async with session.get(f"{self.base_url}/api/tags") as response:
                    if response.status == 200:
                        data = await response.json()
                        models = [model["name"] for model in data.get("models", [])]
                        
                        if self.model in models:
                            logger.info(f"âœ… Ollama ì„œë²„ ì •ìƒ, {self.model} ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥")
                            return True
                        else:
                            logger.warning(f"âš ï¸ {self.model} ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {models}")
                            return False
                    else:
                        logger.error(f"âŒ Ollama ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {response.status}")
                        return False
        except Exception as e:
            logger.error(f"âŒ Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {str(e)}")
            return False

    def _build_rag_prompt(self, question: str, context_documents: List[Dict[str, Any]]) -> str:
        """ì‚¬ë‚´ê·œì • íŠ¹í™” RAG í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        
        # ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œë“¤ì„ ì •ë¦¬
        context_parts = []
        for i, doc in enumerate(context_documents[:1], 1):  # ìƒìœ„ 1ê°œ ë¬¸ì„œë§Œ ì‚¬ìš© (ì†ë„ ìµœìš°ì„ )
            text = doc.get("text", "").strip()
            metadata = doc.get("metadata", {})
            
            # ë¬¸ì„œ ì¶œì²˜ ì •ë³´ êµ¬ì„±
            source_info = []
            if metadata.get("file_name"):
                source_info.append(f"íŒŒì¼: {metadata['file_name']}")
            if metadata.get("sheet_name"):
                source_info.append(f"ì‹œíŠ¸: {metadata['sheet_name']}")
            if metadata.get("cell_address"):
                source_info.append(f"ìœ„ì¹˜: {metadata['cell_address']}")
            
            source = " | ".join(source_info) if source_info else "ì•Œ ìˆ˜ ì—†ìŒ"
            
            if text:
                context_parts.append(f"[ê·œì • {i}] ({source})\n{text}")
        
        context_text = "\n\n".join(context_parts)
        
        # í•œêµ­ì–´ íŠ¹í™” í”„ë¡¬í”„íŠ¸
        prompt = f"""You are a Korean company policy expert. Answer the question based on the provided document in Korean.

Document: {context_text[:200]}

Question: {question}

Please provide a clear and helpful answer in Korean based on the document above:"""

        return prompt

    async def generate_response(self, 
                               question: str, 
                               context_documents: List[Dict[str, Any]] = None,
                               max_tokens: int = 1000) -> Dict[str, Any]:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ LLM ì‘ë‹µ ìƒì„±
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            context_documents: ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ (RAGìš©)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            
        Returns:
            Dict: {
                "response": str,
                "model": str, 
                "context_used": bool,
                "processing_time": float
            }
        """
        import time
        start_time = time.time()
        
        try:
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            if context_documents and len(context_documents) > 0:
                prompt = self._build_rag_prompt(question, context_documents)
                context_used = True
            else:
                prompt = f"ì§ˆë¬¸: {question}\n\në‹µë³€:"
                context_used = False
            
            # Ollama API í˜¸ì¶œ
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,  # ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™” (ì•ˆì •ì„± ìš°ì„ )
                "options": {
                    "temperature": 0.1,  # ë§¤ìš° ê²°ì •ì ì¸ ì‘ë‹µ
                    "top_p": 0.5,       # ë” ì œí•œì 
                    "num_predict": min(max_tokens, 100),  # í† í° ìˆ˜ ë” ê°ì†Œ
                    "stop": ["ì§ˆë¬¸:", "ë¬¸ì„œ:", "\n\n", "ë‹µë³€:", "ë‹¤ìŒ", "ì°¸ê³ :", "ì¶œì²˜:"],
                    "num_ctx": 512,     # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ë” ì œí•œ
                    "repeat_penalty": 1.2,
                    "top_k": 20,        # ì„ íƒ ë²”ìœ„ ì œí•œ
                    "mirostat": 1,      # ì¼ê´€ëœ ì‘ë‹µ ìƒì„±
                    "mirostat_tau": 2.0
                }
            }
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=self.timeout)) as session:
                async with session.post(
                    f"{self.base_url}/api/generate", 
                    json=payload,
                    headers={"Content-Type": "application/json"}
                ) as response:
                    
                    if response.status == 200:
                        # ì¼ë°˜ ì‘ë‹µ ì²˜ë¦¬
                        data = await response.json()
                        response_text = data.get("response", "").strip()
                        
                        processing_time = time.time() - start_time
                        
                        logger.info(f"âœ… LLM ì‘ë‹µ ì™„ë£Œ - ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
                        
                        return {
                            "response": response_text,
                            "model": self.model,
                            "context_used": context_used,
                            "processing_time": processing_time,
                            "prompt_tokens": len(prompt.split()),
                            "completion_tokens": len(response_text.split())
                        }
                    else:
                        error_text = await response.text()
                        raise RuntimeError(f"Ollama API ì˜¤ë¥˜ {response.status}: {error_text}")
                        
        except asyncio.TimeoutError:
            logger.error(f"âŒ LLM ì‘ë‹µ íƒ€ì„ì•„ì›ƒ ({self.timeout}ì´ˆ)")
            raise RuntimeError(f"LLM ì‘ë‹µ ì‹œê°„ ì´ˆê³¼ ({self.timeout}ì´ˆ)")
            
        except Exception as e:
            logger.error(f"âŒ LLM ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise RuntimeError(f"LLM ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")

    async def chat_with_history(self, 
                               messages: List[ChatMessage],
                               context_documents: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        ì±„íŒ… íˆìŠ¤í† ë¦¬ì™€ í•¨ê»˜ ëŒ€í™” ìƒì„±
        
        Args:
            messages: ì±„íŒ… ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
            context_documents: ê²€ìƒ‰ëœ ë¬¸ì„œë“¤
            
        Returns:
            Dict: ì‘ë‹µ ì •ë³´
        """
        # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
        user_messages = [msg for msg in messages if msg.role == "user"]
        if not user_messages:
            raise ValueError("ì‚¬ìš©ì ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        last_question = user_messages[-1].content
        
        # ë‹¨ì¼ ì‘ë‹µ ìƒì„± (í–¥í›„ ì±„íŒ… íˆìŠ¤í† ë¦¬ ì§€ì› í™•ì¥ ê°€ëŠ¥)
        return await self.generate_response(last_question, context_documents)

# ì „ì—­ LLM ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
_llm_service_instance: Optional[OllamaLLMService] = None

def get_llm_service() -> OllamaLLMService:
    """ì „ì—­ LLM ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    global _llm_service_instance
    if _llm_service_instance is None:
        _llm_service_instance = OllamaLLMService(
            host="localhost",
            port=11434,
            model="gemma2:9b",  # Gemma-2-9B ëª¨ë¸ (ìµœì í™” ì ìš©)
            timeout=20
        )
    return _llm_service_instance

async def initialize_llm_service() -> bool:
    """LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ë° ìƒíƒœ í™•ì¸"""
    try:
        llm_service = get_llm_service()
        is_healthy = await llm_service.check_health()
        
        if is_healthy:
            logger.info("ğŸ¤– LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
        else:
            logger.warning("âš ï¸ LLM ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Ollama ì„œë²„ì™€ ëª¨ë¸ì„ í™•ì¸í•˜ì„¸ìš”.")
            return False
            
    except Exception as e:
        logger.error(f"âŒ LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        return False

